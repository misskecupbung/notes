# Kumpulan Catatan Lab dari Cloud Academy

## Create Your First Amazon EC2 Instance (Linux)
Link: https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance/


1. Amazon Elastic Compute Cloud (EC2) is one of the most popular AWS services. 
2. EC2 allows you to launch different types of cloud instances and pay for them with a pay-per-use model. 
3. Menu's in EC2 
  - Additional navigation options are across the top-left of the Dashboard
  - Basic account information, current region, and Support options are across the top-right
  - Navigation to additional EC2 resources and features are located in the left pane
  - Resources section - provides a high-level summary of current EC2 resource usage
  - Launch Instance section - Offers a single click to start the process of launching a new EC2 instance (you'll do that next)
  - Service Health section - Simple and quick way to obtain the high-level service health in current region
  - Additional Information - Context sensitive help on Getting Started (with EC2) or a complete listing of all AWS documentation

4. Secure Shell (SSH) is a cryptographic network protocol for securing data communication. It establishes a secure channel over an insecure network. Common applications include remote command-line login and remote command execution.

5. How to check the EC2 instance metadata, which is only available from within the instance itself. 

6. Instance metadata is data about your instance that you can use to configure or manage the running instance. In order to obtain the instance metadata you will use the curl utility. cURL (Client URL) is a free, open-source project, and already loaded on your instance, supported protocol such as HTTP. 

7. Note: The IP address used below (169.254.169.254) is a special use address to return metadata information tied to EC2 instances. The following steps use the Instance Metadata Service Version 1 (IMDSv1) method to access the instance metadata. By default, both IMDSv1 and IMDSv2 can be used on most AMIs. For more information regarding these methods, please reference the following AWS documentation. 

```bash
curl -w "\n" http://169.254.169.254/latest/meta-data/
```

8. AWS bills EC2 usage per second in most cases. The specific instance and the data on the root volume (system disk) is not recoverable (by default) however. So be sure you don't need it before terminating an instance. If you stop an instance, you can start it again later (and access data on all the disks). In this lab step, you will terminate a running EC2 instance.


## Introduction to Virtual Private Cloud (VPC)
Link: https://cloudacademy.com/lab/introduction-virtual-private-cloud-vpc/


1. Amazon Virtual Private Cloud (VPC) lets you provision a logically isolated section of the Amazon Web Services (AWS) Cloud where you can launch AWS resources in a virtual network that you define. 

2. You have complete control over your virtual networking environment, including the selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. 

3. Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center with the benefits of using the scalable infrastructure of AWS. It is logically isolated from other virtual networks in the AWS cloud.

4. Amazon creates the requested VPC and the following linked services:
  - a DHCP options set  (this set enables DNS for instances that need to communicate over the VPC's Internet gateway) 
  - a Route Table  (it contains a set of rules, called routes, that are used to determine where network traffic is directed) 
  - a Network ACL  (it is a list of rules to determine whether traffic is allowed in or out of any subnet associated with the network ACL)

A VPC subnet is a range of IP addresses in your VPC. You can add one or more subnets in each Availability Zone, but each subnet must reside entirely within one Availability Zone and cannot span zones. Availability Zones are distinct locations that are engineered to be isolated from failures in other Availability Zones. By launching instances in separate Availability Zones, you can protect your applications from the failure of a single location.

5. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the Internet. It imposes no availability risks or bandwidth constraints on your network traffic. 

6. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic and to perform network address translation (NAT) for instances that have been assigned public IP addresses.

7. To use an internet gateway your subnet's route table must contain a route that directs internet-bound traffic to the internet gateway. You can scope the route to all destinations not explicitly known to the route table (0.0.0.0/0), or you can scope the route to a narrower range of IP addresses; for example, the public IP addresses of your companyâ€™s public endpoints outside of AWS, or the Elastic IP addresses of other Amazon EC2 instances outside your VPC. If your subnet is associated with a route table that has a route to an internet gateway, it's known as a public subnet.

8. An Elastic IP address (EIP) is a static and public IP address that you can associate with an EC2 instance. EIPs have the benefit of not changing when you stop and start an EC2 instance, whereas the default public IP that comes with an EC2 instance may change. This gives you the benefit of a reliable IP address to associate with your EC2 instance. In this Lab Step you will allocate an EIP and associate it with your EC2 instance.



## Introduction to DynamoDB
Link: https://cloudacademy.com/lab/introduction-dynamodb/

1. Amazon DynamoDB is a NoSQL Database in the cloud, suitable for anyone needing a reliable and fully managed NoSQL solution. DynamoDB is designed to provide automated storage scaling and low latency. It is particularly useful when your application must read and store massive amounts of data and you need speed and reliability (Amazon works with replicas of your database in three different Availability Zones). Amazon DynamoDB is totally managed. You simply select an AWS region, define the needed indexes for each table you will create, and Amazon takes care of everything else.

2. PartiQL is a SQL (Structured Query Language) compatible language for Amazon DynamoDB. As well as querying tables, you can use it to insert new items and update existing ones.

3. PartiQL supports most standard features of SQL which means you can query, select, and sort your data in sophisticated ways.

4. Typically, using the Amazon DynamoDB Console to query items is useful for one-off reports and debugging or troubleshooting. Like most databases, DynamoDB can be accessed programmatically by other systems and software applications through either the AWS SDK (software development kit) or DyanmoDB's HTTP API (application programming interface).

## Configuring a Static Website With S3 And CloudFront
Link: https://cloudacademy.com/lab/configuring-static-website-s3-and-cloudfront/


1. You can easily and inexpensively use Amazon Web Services (AWS) to host a website that uses client-side technologies (such as HTML, CSS, and JavaScript) and does not require server-side technologies (such as PHP and ASP.NET). This type of site is called a static website and is used to display content that does not change frequently.

2. Amazon Simple Storage Service (Amazon S3) is an object storage service from AWS. It's well-known for its scalability, availability, security features, and performance. Object storage is a ubiquitous need and Amazon S3 is often a part of any non-trivial solution built in AWS.

3. You have added a unique number to the bucket name because Amazon S3 bucket names must be unique regardless of the AWS region in which the bucket is created.

4. A bucket name must also be DNS compliant. Here are some of the rules it must adhere to:
  - They must be at least 3 and no more than 63 characters long.
  - They may contain lowercase letters, numbers, periods, and/or hyphens.
  - Each label must start and end with a lowercase letter or a number.
  - They cannot be formatted as an IP address (for example, 192.168.1.1).

5. The Properties tab allows you to enable and disable various Amazon S3 bucket features, including:
  - Bucket Versioning: Old versions can be kept when objects are updated
  - Default encryption: A bucket can be configured to encrypt all objects by default
  - Server access logging: Web-server style access logs can be enabled
  Requester pays: When enabled, the entity downloading data from this bucket will pay data transfer costs incurred

6. Bucket policy to s3
```bash
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AddPerm",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::YOUR_BUCKET_NAME/*"
    }
  ]
}
```

7. Amazon CloudFront is a global Content Delivery Network (CDN) that delivers data securely and efficiently. CloudFront pulls your website out to the edge of the network, reducing latency when accessed from different global locations.


## Introduction to the Elastic File System
Link: https://cloudacademy.com/lab/introduction-elastic-file-system/


1. Amazon Elastic File System (Amazon EFS) provides simple, scalable file storage for use with Amazon EC2 instances in the AWS Cloud. With Amazon EFS, storage capacity is elastic, growing and shrinking automatically as you add and remove files - so your applications have the storage they need, when they need it.

2. Amazon EFS has a simple web services interface that allows you to create and configure file systems quickly and easily. The service manages all the file storage infrastructure for you, avoiding the complexity of deploying, patching, and maintaining complex file system deployments. 


## Managing Instance Volumes Using EBS
Link: https://cloudacademy.com/lab/managing-instance-volumes-using-ebs/

1. Amazon Elastic Block Store (Amazon EBS) provides persistent block-level storage volumes for use with Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure. This provides high availability and durability. Amazon EBS volumes offer consistent, low-latency performance needed to run your workloads.

2. After writing data to an Amazon EBS volume, you can periodically create a snapshot of the volume to use as a baseline for new volumes or for data backup. If you make periodic snapshots of a volume, the snapshots are incremental so that only the blocks on the device that have changed after your last snapshot are saved in the new snapshot. Even though snapshots are saved incrementally, the snapshot deletion process is designed so that you need to retain only the most recent snapshot in order to restore the volume.

3. Snapshots occur asynchronously and the status of the snapshot is "pending" until the snapshot is complete.

4. You can take a snapshot of an attached volume that is in use. However, snapshots only capture data that has been written to your Amazon EBS volume at the time the snapshot command is issued. This might exclude any data that has been cached by any applications or the operating system. If you can pause any file writes to the volume long enough to take a snapshot, your snapshot should be complete. However, if you can't pause all file writes to the volume, you should unmount the volume from within the instance, issue the snapshot command, and then remount the volume to ensure a consistent and complete snapshot. You can remount and use your volume while the snapshot status is "pending".

5. To create a snapshot for Amazon EBS volumes that serve as root devices, you should stop the instance before taking the snapshot.


## Working with Amazon EC2 Auto Scaling Groups and Network Load Balancer
Link: https://cloudacademy.com/lab/working-amazon-ec2-auto-scaling-groups/


1. The Amazon Web Services (AWS) Auto Scaling service automatically adds or removes compute resources allocated for your cloud application, in response to changes in demand. For applications configured to run on a cloud infrastructure, scaling is an important part of cost control and resource management.

2. Scaling is the ability to increase or decrease the compute capacity of your application either by changing the number of servers (horizontal scaling) or by changing the size of the servers (vertical scaling).

3. Auto Scaling helps you maintain application availability and allows you to scale your Amazon EC2 capacity up or down automatically according to the defined conditions. You can use Auto Scaling to help ensure that you are running your desired number of Amazon EC2 instances. Auto Scaling can also automatically increase the number of Amazon EC2 instances during demand spikes to maintain performance and decrease capacity during lulls to reduce costs. Auto Scaling is well suited to applications that have stable demand patterns, or that experience hourly, daily, or weekly variability in usage.

4. Groups: Your EC2 instances are organized into groups so that they can be treated as a logical unit for the purposes of scaling and management. When you create a group, you can specify its minimum, maximum, and desired number of EC2 instances. 

5. Launch configurations: Your group uses a launch configuration as a template for its EC2 instances. When you create a launch configuration, you can specify information such as the AMI ID, instance type, key pair, security groups, and block device mapping for your instances. 

6. Launch template: A launch template is similar to a launch configuration, in that it specifies instance configuration information. ... However, defining a launch template instead of a launch configuration allows you to have multiple versions of a template. With versioning, you can create a subset of the full set of parameters and then reuse it to create other templates or template versions.


7. Auto Scaling groups are commonly paired with load balancers that evenly distribute requests across all the instances in the group. In this Lab, you will create a load balancer before creating an Auto Scaling group so that the Auto Scaling group can be configured to work with the load balancer at creation time.

8. Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple Amazon EC2 instances. They enable you to achieve greater fault tolerance in your applications and seamlessly provide the correct amount of load balancing capacity needed in response to incoming application requests.

9. ELB detects unhealthy instances within a pool and automatically reroutes traffic to healthy instances until the unhealthy instances have been restored. Elastic Load Balancers can be enabled within a single Availability Zone or across multiple zones for greater consistent application performance.

There are several ELB load balancers to choose from. The network load balancer is a network layer (layer-4) load balancer operating on TCP connections and UDP. It can scale to millions of requests per second and is a more modern alternative to the classic load balancer (also a layer-7 load balancer). With a network load balancer, backend targets are organized into target groups which the network load balancer distributes traffic across. You will create a network load balancer in this lab step.

10. You must enable Cross-zone load balancing to achieve the highest level of availability. Without enabling this feature, clients could cache the DNS address of the load balancer node in one availability zone and that node would only distribute requests to instances within the availability zone. Cross-Zone Load Balancing allows every load balancer node to distribute requests across all availability zones, although for the Network Load Balancer there are data transfer charges when this feature is enabled. (There are no data charges for other types of load balancers)

11. A Launch Template is a template that an Auto Scaling group uses to launch Amazon EC2 instances. If you've launched an individual EC2 instance before, you've already walked through the process of defining compute characteristics such as the instance type, security groups, and configuration scripts. A launch template allows you to define these same characteristics, which are then applied to any instances launched in the Auto Scaling group that references the Launch Template. The Launch Template essentially contains the blueprint or DNA for the exact type of instance that should be launched. Hence, when auto-scaling, each instance is guaranteed to be just like the last one. It's repeatable, scalable, and reliable.

12. When you create the Launch Template you will include information such as the Amazon machine image ID (AMI) to use for launching the EC2 instance, the instance type, key pairs, security groups, and block device mappings, among other configuration settings. When you create your Auto Scaling group, you must associate it with a Launch Template.

13. An Auto Scaling group is a representation of multiple Amazon EC2 instances that share similar characteristics and that are treated as a logical grouping for the purposes of instance scaling and management. For example, if a single application operates across multiple instances, you might want to increase or decrease the number of instances in that group to improve the performance of the application. You can use the Auto Scaling group to automatically scale the number of instances or maintain a fixed number of instances. You create Auto Scaling groups by defining the minimum, maximum, and the desired number of running EC2 instances the group must have at any given point of time.

14. An Auto Scaling group starts by launching the minimum number (or the desired number, if specified) of EC2 instances and then increases or decreases the number of running EC2 instances automatically according to the conditions that you define. Auto Scaling also maintains the current instance levels by conducting periodic health checks on all the instances within the Auto Scaling group. If an EC2 instance within the Auto Scaling group becomes unhealthy, Auto Scaling terminates the unhealthy instance and launches a new one to replace the unhealthy instance. This automatic scaling and maintenance of the instance in an Auto Scaling group is the core value of the Auto Scaling service. It's what puts the "elastic" in EC2. 
 

15. Auto-scaling groups allow you to scale out (add more instances) or scale in (remove instances) based on metrics such as:
  - Average CPU utilization
  - Network traffic (ingress and egress)
  - Application Load Balancer request counts

16. It's possible to create your own metrics and use these to decide when to scale, such metrics can be populated in CloudWatch directly from the application rather than inspecting the instance's resource usage. This is useful when the point that you need to scale at is determined by something unrelated to the instance, such as reaching a database connection limit or some other application-specific bottleneck.

17. By default, the Metric type is Average CPU utilization, and the Target value is 50 percent. Leave these values unchanged.


18. Performing end-to-end tests to make sure everything is working as you think it should is very important. Although this may be an automated procedure, often a quick sanity test by other individuals and/or groups directly from the AWS Console is also helpful. This lab step will point out a few ways to test that your Launch Configuration is working in conjunction with the Auto Scaling group and CloudWatch Alarm (which uses AWS Simple Notification Service (SNS)).

```bash
stress --cpu 2 --io 1 --vm 1 --vm-bytes 128M --timeout 5m
```

## Create Your First Amazon RDS Database
Link: https://cloudacademy.com/lab/create-your-first-amazon-rds-database/


1. Amazon Relational Database Service (Amazon RDS) is a web service that makes it easy to set up, operate, and scale a relational database in the cloud. You can migrate existing applications and tools that utilize a relational database to Amazon Web Services because Amazon RDS offers access to the capabilities of a MySQL, Oracle, SQL Server, and the PostgreSQL database engine. Amazon RDS provides cost-efficient and resizable capacity while managing time-consuming database administration tasks. This efficiency frees you to focus on your applications and business.

2. Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. Before launching actual RDS instances, you need to configure a DB Subnet Group.

3. Subnets are segments of a VPC's IP address range that allow you to group your resources based on security and operational needs. A DB Subnet Group is a collection of subnets (typically private) that you create in a VPC and designate for your DB instances. Each DB subnet group should have subnets in at least two Availability Zones in a given region. Note that SQL Server Mirroring with a SQL Server DB instance requires at least 3 subnets in distinct Availability Zones.

4. When creating a DB instance in a VPC, you must select a DB subnet group. Amazon RDS uses that DB subnet group and your preferred Availability Zone to select a subnet and an IP address within that subnet to associate with your DB instance. When Amazon RDS creates a DB instance in a VPC, it assigns a network interface to your DB instance by using an IP address selected from your DB Subnet Group. If the primary DB instance of a Multi-AZ deployment fails, Amazon RDS can promote the corresponding standby and subsequently create a new standby using an IP address from an assigned subnet in one of the other Availability Zones.

5. The rules of a Security Group control the inbound traffic that's allowed to reach the instances that are associated with the security group and the outbound traffic that's allowed to leave them. By default, security groups allow all outbound traffic and deny all inbound traffic.


6. Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. Amazon RDS is designed for developers or businesses who require the full features and capabilities of a relational database or who wish to migrate existing applications and tools that utilize a relational database. It gives you access to the capabilities of a MySQL, Oracle, Microsoft SQL Server, MariaDB, Amazon Aurora, or PostgreSQL database engine.

7. The RDS service is fully managed by Amazon. RDS will make sure that the database software stays up-to-date with the latest patches and any faulty compute instance powering your database deployment will be automatically replaced in the event of a hardware failure. You can automatically or manually create database snapshots and easily scale your infrastructure up or down using the AWS Management Console.

8. For production workloads, you can consider using provisioned IOPS as the Storage type to guarantee consistent throughput levels. Storage autoscaling can also be useful in production to avoid manually scaling up the database's storage when it breaches a certain threshold. Neither is needed for this lab. 

9. Session Manager is part of AWS Systems Manager suite of tools for gaining operational insights and taking action on AWS resources. Session Manager gives you browser-based shell access to EC2 instances running the Systems Manager agent. Both Windows and Linux instances are supported. Session manager provides secure access to instances without the need to distribute passwords or SSH keys. Session Manager also allows you to connect to instances without having to open any inbound ports. All communication is encrypted and IAM policies can restrict access to sessions running in Session Manager.


### Advanced Roles and Groups Management Using IAM
Link: https://cloudacademy.com/lab/advanced-roles-and-groups-management-using-iam/

1. AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources for your users. You can specify permissions to a single user or you can use groups to specify permissions for a collection of users, which can make those permissions easier to manage for those users. Furthermore, you can use a Role to grant authorization to AWS resources without any credentials (password or access keys) directly associated with it. In this lab, you will learn the recommended AWS security best practices.

2. AWS Identity and Access Management (IAM) enables you to securely control access to AWS services and resources for your users. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources.

In AWS, you can designate an IAM role to attach to an EC2 instance when launching the instance, or any time after. Attaching an IAM role to an instance allows you to manage permissions for instances centrally with IAM. 


### Securing your VPC using Public and Private Subnets
Link: https://cloudacademy.com/lab/securing-your-vpc-using-public-and-private-subnets-with-network-acl/


Lab description

In this lab, you will design a VPC with a public subnet, a private subnet, and a Network Address Translation (NAT) device in the public subnet. 

A NAT device enables instances in the private subnet to initiate outbound traffic to the Internet. This scenario is common when you have a public-facing web application while maintaining back-end servers that aren't publicly accessible. 

A common example is a multi-tier website, with the web servers in a public subnet, and the database servers in a private subnet. You can set up security and routing allowing the web servers to communicate with the database servers. The instances in the public subnet can send outbound traffic directly to the Internet, whereas the instances in the private subnet cannot. The instances in the private subnet can access the Internet via the NAT Gateway in the public subnet. In this Lab, you will also increase the network security using a network access control list (NACL), which is an optional layer of security that acts as a firewall for controlling traffic in and out of a subnet. After completing this Lab, you might consider setting up network ACLs with rules similar to your security groups, in order to add an additional layer of security to your VPC.



Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network you have defined. This virtual network closely resembles a traditional network that you would operate in your own data center with the benefits of using the scalable infrastructure of AWS. It is logically isolated from other virtual networks in the AWS cloud.

In this lab, you will create a new VPC using the AWS Management Console. Once created, you will create other EC2 and VPC resources mimicking a common two-tiered (front-end and back-end) architecture in the cloud.

Introduction

An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the Internet. It imposes no availability risks or bandwidth constraints on your network traffic. An Internet gateway serves two purposes:

    Provide a target in your VPC route tables for Internet-routable traffic
    Perform network address translation (NAT) for instances that have been assigned public IP addresses. (Note: It does not do this for instances with private IP addresses.)

In this lab, you will create an Internet Gateway and associate it to a VPC.

A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. Each route in a table specifies a destination CIDR and a target (for example, traffic destined for 172.16.0.0/12 is targeted for the virtual private gateway).  If a subnet has a route with the destination (0.0.0.0/0) and Internet Gateway as the target, the subnet is known as a public subnet. You can create a custom route table for your VPC using the Amazon VPC console.

A bastion host is typically a host that sits inside your public subnet for the purposes of SSH (and/or RDP) access. You can think of it as a host for gaining secure access to resources in your VPC from the public internet. Bastion hosts are sometimes referred to as jump servers, as you jump to one, then back out of it.

Once you access a bastion host (for example, by using SSH to log into it), in order to access other instances you must either set up SSH port forwarding or copy your SSH key material to the bastion host. The latter is not ideal for security reasons in a production environment. If you require Windows connectivity, then setting up Remote Desktop Gateway instead of SSH port forwarding is recommended. This lab step assumes SSH connectivity to Linux instances.

In this lab step, you will create an EC2 instance that will serve as both an observer instance that you can run various tests from and a bastion host.



A Network Access Control List (NACL) is an optional layer of security that acts as a firewall for controlling traffic in and out of a subnet. The network access control list is a numbered list of rules, evaluated in order, to determine whether traffic is allowed in or out of any associated subnet. NACLs are stateless which means they cannot tell if a message is in response to a request that started inside the VPC, or if it is a request that started from the internet. Hence, a NACL is better suited for private subnets. For public subnets, using security groups is recommended without NACLs.

The VPC comes with a modifiable default network ACL and each subnet must be associated with a network ACL. If you do not explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL that allows all inbound and outbound traffic:  

In this lab, you will create a Network Access Control List for your private subnet.

Introduction

In this lab step, you will create inbound and outbound rules for your private Network Access Control List (NACL). A NACL is a numbered list of rules that are evaluated in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL. As a best practice, you will start by creating rules with rule numbers that are multiples of 100. This can help with organization if you need to insert new rules later on, as there is room within the numbering scheme.

Warning: Although configuration is not difficult, it is easy to make mistakes or typos. For example, an incorrect digit for a CIDR block can break the lab. Take your time configuring the rules with the instructions below.


Introduction

When choosing between a NAT Gateway and a NAT instance to handle your network address translations, there are a few key differences to consider. Some examples include maintenance, availability, and performance.

A NAT instance is managed by you, which includes software installation, updates, and patching. It also requires additional scripting to manage its availability and failover between instances. Its performance relies heavily on a generic Amazon Machine Image (AMI) that is configured to perform NAT. 

A NAT Gateway, on the other hand, is managed by AWS which means you do not need to perform any maintenance. It's highly available with a NAT gateway in each Availability Zone to improve redundancy. As for performance, the software used by a NAT Gateway is optimized for handling NAT traffic.

To read more about the differences between a NAT Gateway and a NAT instance, refer to the following documentation:

    Compare NAT gateways and NAT instances

In addition to the differences mentioned above, it's important to note that the  Amazon Linux AMI has ended its standard support on December 31, 2020. Going forward, AWS recommends that you migrate to using NAT Gateways or create and maintain your own NAT AMI on Amazon Linux 2. 

In this lab step, you will create a NAT Gateway that will be used by the EC2 instance in your private subnet to access the public internet. You will also revisit the route table associated with the private subnet and update the target entry to point to this gateway.


Let's look at the high-level lab environment diagram again, and go over some key points to tie this Lab together. Time and interest permitting, use the AWS Management Console to look up and/or confirm various settings as you read through this lab step. Realize that different organizations might configure things differently in accord with their security goals and policies. For example, increased or relaxed security on various inbound or outbound rules, etc. 

alt

The VPC has been configured with two subnets, a public subnet, and a private subnet. If a subnet's traffic is routed to an Internet gateway, the subnet is known as a public subnet. If a subnet doesn't have a route to the Internet gateway, the subnet is known as a private subnet. Instances launched in a private subnet do not have publicly routable internet addresses either.

Both subnets have a route table associated with them. Instances on the public subnet route internet traffic through the internet gateway. The private subnet routes internet traffic through the NAT device (gateway or instance).

Each instance launched in either subnet has its own security group with inbound and outbound rules, to guarantee access is locked down to specific ports and protocols. For example, private instances on the private subnet allow any outbound traffic but only allow SSH access from the bastion host. As another example, although the NAT device is in the public subnet, it cannot be reached from the internet. It has an inbound rule that only grants instances from the private security group (private instances) access. Note that you might allow SSH access from your personal IP address or specific administrator's as well, or perhaps grant ICMP (ping) access during setup and troubleshooting efforts.

In addition to security groups, the private subnet also has a network access control list (NACL) as an added measure of security. NACL's allow for inbound and outbound rules, specified in priority order. They are set up as implicit allow rules. If none of them are matched, all other traffic is denied. This private subnet NACL in this Lab allowed for SSH inbound traffic from the public subnet only. The outbound rules for the private NACL allowed for HTTP/S access to anywhere. This was proven to work in the Lab by performing operating system updates once the NAT device was in place. The private route table sends the traffic from the instances in the private subnet to the NAT device in the public subnet. The NAT device sends the traffic to the Internet gateway for the VPC. The traffic is attributed to the Elastic IP address of the NAT device. 

### Deploy a PHP application using Elastic Beanstalk
Link: https://cloudacademy.com/lab/deploy-php-application-using-elastic-beanstalk/

Lab description
Elastic Beanstalk: Deploy and manage applications in the AWS cloud

AWS Elastic Beanstalk is an easy way to deploy and scale applications written in Python, Ruby, Java, Node.js, Go, or PHP in familiar environments like Apache, Nginx, Passenger, and IIS, without worrying about the infrastructure that runs those applications.

In this lab, you will learn how to upload your code and deploy it with monitoring, auto-scaling, and load-balancing. Elastic Beanstalk is also free - you only pay for the AWS resources your application needs to run. Elastic Beanstalk lets you directly control the underlying AWS resources if required.
Lab Objectives

Upon completion of this lab, you will be able to:

    Create Elastic Beanstalk applications and environments
    Manage and monitor Elastic Beanstalk applications and environments
    Understand different application update strategies in Elastic Beanstalk

Lab Prerequisites

You should be familiar with:

    Basic Amazon EC2 and Virtual Private Cloud (VPC) concepts


Since you will be deploying an application to a production-ready environment and performing a rolling update in this Lab, it makes sense to use the best tool AWS makes available for the job. For controlled deployments and efficient deployment services of code on EC2 instances, Elastic Beanstalk provides a superior interaction model and developer tools experience.

## Introduction to AWS Lambda
Link: https://cloudacademy.com/lab/introduction-aws-lambda/

ntroduction

AWS Lambda Logo AWS Lambda is a service released for Developer Preview in April 2015. Some examples of the kinds of things you can achieve with Lambda are designing advanced materialized views out of DynamoDB tables, reacting to uploaded files on S3, and processing SNS messages or Kinesis streams.

In short, you can write a stateless Lambda function that will be triggered to react to certain events (or HTTP endpoints). In a way, it's the same Platform as a Service (PaaS) vein as Heroku. The key difference is in how data (events for Lambda, web requests for Heroku) is delivered to your code and the level of granularity you can achieve. In both cases, you write code that accepts some constraints in exchange for not having to do any provisioning, updating, or management of the underlying resources.

Lambda opens up all kinds of new possibilities and can lower your costs at the same time. When running a job processing server in EC2, you are charged for compute-time as long as your instance is running. Contrast that with Lambda, where you are only charged while actually processing a job, on a 1ms basis. Basically, you never pay for idle time.

This makes Lambda a great fit for spiky or infrequent workloads because it scales automatically and minimizes costs during slow periods. The event-based model Lambda provides makes it perfect for providing a backend for mobile clients, IoT devices, or adding no-stress asynchronous processing to an existing application, without worrying too much about scaling your compute power.

The current computing landscape for AWS looks crowded at first glance with EC2, Elastic Beanstalk, EKS, Lambda, Simple Workflow Service, and more vying for your workload. Before you start on this Lab, you should understand where Lambda fits in.

EC2 is the most basic service, as it only provides the instance with a base image while you supply the automation, configuration, and code to run. It's the most flexible option, but it also requires the most work from you.

Lambda is the complete opposite in that it handles provisioning, underlying OS updates, monitoring, and failover transparently. You only need to provide the code that will run and specify what events should trigger your code. Scaling a Lambda function happens automatically; AWS provisions more instances as needed and only charges you for the time your function runs. 

Elastic Beanstalk is a PaaS that lets you deploy code without worrying about the underlying infrastructure. However, compared to Lambda, it does provide more choices and controls. You can deploy complete applications to Elastic Beanstalk using a more traditional application model compared to deploying individual functions in Lambda.

Amazon Elastic Container Service (Amazon ECS) and Amazon Elastic Container Service for Kubernetes (Amazon EKS) are centered around containers compared to the individual functions of Lambda. ECS and EKS require less managerial overhead compared to running containers on EC2 instances, but generally require some operational expertise. Lambda is ideal for developers who just want to focus on their code.

Simple Workflow Service is a coordination service, and you must provision workers to complete your tasks.

In this Lab, you will set up a Lambda function, learn how to test code in the AWS Console, and discuss different event sources for bringing data into Lambda. Functions like the ones you will write in this Lab can be used to help keep data in sync, fan out writes to users' news feeds, or update indexes in DynamoDB and other databases. 

A Lambda function is a small unit of computation that can execute in parallel, in a stateless fashion. This is only partially true though since you can potentially have some shared initialization code, at the container level.

In this lab step, you will create a new function and play with it for the rest of this lab.

The Internet of Things (IoT) has a wealth of applications for AWS Lambda because most devices are low-power by design. They are also hard to update and need to be cheap enough to be embedded in fridges, toasters, televisions, robots, cars, and more. All this, of course, without driving the cost of each device too high.

You will use a microwave as an example. It will send you the cooking time, the button presses used to start the session and information about presets usage. AWS Lambda can receive these messages over Amazon Simple Notification Service (SNS), and you can hook as many functions as you like to the same SNS topic. (For large-scale IoT installations, you should consider AWS IoT)

Lambda functions are intended to be very simple. This makes testing, debugging, updating, and maintaining Lambda backends super easy. In the example function, you are going to collect information about if the user stopped the microwave before time ran out and if a preset was used. This sort of information can help you improve the presets by collecting real-world data. 

The code you used in the previous lab step reads an incoming SNS message and parses the JSON body to be analyzed (line 4), then checks if the microwave was stopped before the requested number of seconds (line 5), and finally if a preset was used or not (lines 6-8):



In this lab step, you tested the Lambda function you created by providing a test event. The event was configured to test a user stopping a microwave early without using a preset. The function only returns a string based on the conditions, but you could be counting incidents in DynamoDB, sending notifications to users, or any number of other things. Of course, the cooking time on a microwave isn't incredibly interesting data. Cheap sensors make it feasible to collect all sorts of more interesting data and use Lambda to act on it in near-real-time.

If you have extra time in your Lab session, you can try modifying the test event to exercise different code paths and check the logs after running the test to confirm everything works as expected. Be sure to leave a few minutes for the final lab step that validates the work you did in this Lab.


## Create Your First Amazon S3 Bucket
Link: https://cloudacademy.com/lab/create-your-first-amazon-s3-bucket/

Amazon Simple Storage Service (S3) provides secure, durable, and highly scalable object storage. To upload data such as photos, videos, and static documents, you must first create a logical storage bucket in one of the AWS regions. Then you can upload any number of objects to it. Buckets and objects are resources, and Amazon S3 provides both APIs and a web console to manage them.

Amazon S3 can be used alone or together with other AWS services such as Amazon EC2, Amazon Elastic Block Store (Amazon EBS), and Amazon Glacier, as well as third-party storage repositories and gateways. Amazon S3 provides cost-effective object storage for a wide variety of use cases including web applications, content distribution, backup and archiving, disaster recovery, and big data analytics.

This Lab guides you through the bucket creation process and its first usage. The Cloud Academy Labs Engine constantly checks the Lab environment, giving you instant feedback on your work.
Learning Objectives

Upon completion of this beginner-level Lab you will be able to:

    Create an S3 bucket
    Create a folder within an S3 bucket
    Upload content to S3
    Change permissions to allow public access to content
    Set metadata on an S3 bucket
    Delete an S3 bucket and its contents
Important! Bucket names must be globally unique, regardless of the AWS region in which you create the bucket. Buckets must also be DNS-compliant.

The rules for DNS-compliant bucket names are:

    Bucket names must be at least 3 and no more than 63 characters long.
    Bucket names can contain lowercase letters, numbers, periods, and/or hyphens. Each label must start and end with a lowercase letter or a number.
    Bucket names must not be formatted as an IP address (for example, 192.168.1.1).


Introduction

When you upload a folder from your local system or another machine, Amazon S3 uploads all the files and subfolders from the specified local folder to your bucket. It then assigns a key value that is a combination of the uploaded file name and the folder name. In this lab step, you will upload a file to your bucket. The process is similar to uploading a single file, multiple files, or a folder with files in it.

In order to complete this lab step, you have to upload the cloudacademy-logo.png file from your local file storage into an S3 folder you created earlier.

Download the Cloud Academy logo from the following location: https://s3-us-west-2.amazonaws.com/clouda-labs-assets/scripts/s3/cloudacademy-logo.png (If the image is not downloaded for you, simply right-click the image and select Save image as to download it to your local file system.)


Turning off Block all public access does not automatically make objects in an Amazon S3 bucket public. There are several ways of explicitly granting public access including:

    Bucket policies
    IAM policies
    Access control lists
    Pre-signed URLs


## Introduction to CloudWatch
Link: https://cloudacademy.com/lab/introduction-to-cloudwatch/


In this Lab, you will explore CloudWatch, a monitoring service provided by AWS. CloudWatch makes it possible to monitor nearly anything inside or outside AWS. Common usage is to monitor EC2 instances for CPU and memory utilization, ephemeral or EBS volume disk usage and throughput, as well as network statistics. CloudWatch also enables you to configure alarms, so that when thresholds are violated for specific metrics a notification and/or action is automatically triggered.
Prerequisites

Although this is a beginner level Lab, you should be familiar with:

    Using the AWS Console
    EC2 basics
    Conceptual understanding of the type of metrics typically monitored (such as CPU, storage, and network resources)

Learning Objectives

Upon completion of this Lab, you will be able to:

    Explain key CloudWatch concepts and default EC2 metrics
    Create and test an alarm
    Add actions to an alarm
    Install monitoring tools as EC2 User Data to send custom metrics to CloudWatch
    Navigate about in CloudWatch and explore many other possibilities for monitoring AWS resources


Introduction

Before using CloudWatch it is important to understand a few key concepts. Spending a few minutes exploring with the CloudWatch console is also helpful.

 
Instructions

1. AWS has done an excellent job defining CloudWatch key concepts. Read the abbreviated excerpt from their official documentation below to obtain an understanding of Metrics, Namespaces and Alarms: 

    Metrics

    A metric is the fundamental concept in CloudWatch and represents a time-ordered set of data points. These data points can be either your custom metrics or metrics from other services in AWS. You or AWS products publish metric data points into CloudWatch and you retrieve statistics about those data points as an ordered set of time-series data. Metrics exist only in the region in which they are created.

    Think of a metric as a variable to monitor, and the data points represent the values of that variable over time. For example, the CPU usage of a particular Amazon EC2 instance is one metric, and the latency of an Elastic Load Balancing load balancer is another.

    Namespaces

    CloudWatch namespaces are containers for metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.

    Note: In this lab you will see namespaces that AWS has created for you, and a custom namespace created by the steps performed in this lab.

    Alarms

    You can use an alarm to automatically initiate actions on your behalf. An alarm watches a single metric over a specified time period, and performs one or more specified actions, based on the value of the metric relative to a threshold over time. The action is a notification sent to an Amazon SNS topic or an Auto Scaling policy. You can also add alarms to dashboards.

    Alarms invoke actions for sustained state changes only. CloudWatch alarms will not invoke actions simply because they are in a particular state. The state must have changed and been maintained for a specified number of periods.

The interested student can take a look at the full version of the documentation here. Due to time constraints, you should look at additional documentation once you have completed the lab.

What metrics are listed on the All metrics tab depends on a couple of factors:

    How quickly you arrived at this view after starting your lab. This lab creates an EC2 instance and EBS volume when you start the lab. After a couple of minutes of delay, metrics for the EC2 and EBS namespaces are included.
    How recently your Cloud Academy AWS account has been used to complete other Cloud Academy labs. If the AWS account you logged in to recently completed other labs, you may see namespace related to metrics collected in those labs.

CloudWatch does  not monitor everything out of the box. For example, for EC2 instances it does not monitor things like disk space or memory usage. In this Lab Step you will get a better idea of what CloudWatch monitors by default for EC2 instances, but this time you'll use the EC2 console not the CloudWatch console.


Note: If you don't see an instance yet, it's possible that it's still provisioning in the background. Refresh the page every minute or so until it appears.

These are the standard metrics that CloudWatch monitors for all your EC2 instances. Please refer to the documentation for details. (Due to possible time constraints, please look up additional information in the documentation after completing this lab.)

You should be aware that all the metrics in this tab related to Disk (Disk Reads, Disk Read Operations, Disk Writes, Disk Write Operations) pertain to ephemeral storage disks. Those metrics will not represent anything if you have launched an EBS backed instance. To see the metrics related to EBS volumes you need to look elsewhere. Next you will take a look at the metrics of the EBS volume for this particular instance.

Note: Ephemeral storage is also known as instance storage. It is temporary storage that is added to your instance, unlike EBS which is an attached volume that is permanent in nature.


alt

As you can see, Amazon does quite a bit out of the box with respect to monitoring EC2 Instances and EBS volumes. However, you can enable Detailed Monitoring for even more control over the monitoring frequency of EC2 instances. CloudWatch monitors EC2 instances every 5 minutes by default. If you need more frequent monitoring, you can enable CloudWatch's Detailed Monitoring feature to monitor your instances every minute. You can enable Detailed Monitoring during the instance launch or change it anytime afterwards. Note: Detailed Monitoring does come with an associated cost.

 
Summary 

In this Lab Step you learned about the standard metrics offered by AWS when monitoring EC2 Instances and EBS volumes. You also learned how to enable (or disable) Detailed Monitoring. Detailed Monitoring increases the monitoring frequency of EC2 instances giving you access to metrics at a quicker rate. You can learn more about metrics by reading the Amazon EBS Metrics and Dimensions documentation (after the lab is completed). 

```bash

#!/bin/bash
yum install -y perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64
wget http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.2.zip
unzip CloudWatchMonitoringScripts-1.2.2.zip
rm CloudWatchMonitoringScripts-1.2.2.zip
echo "*/1 * * * * /aws-scripts-mon/mon-put-instance-data.pl --mem-util --disk-space-util --disk-path=/ --from-cron" > monitoring.txt
crontab monitoring.txt
rm monitoring.txt
```

An alarm watches for a metric to go beyond an allowable value range when monitored over time. If violated the alarm's state is changed. There are three possibles states for an alarm:

    OKâ€”The metric is within the defined threshold
    In alarmâ€”The metric is outside of the defined threshold
    Insufficient dataâ€”The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state


In addition to creating an alarm from the CloudWatch console, you can also create one from the EC2 console. The mechanics are similar, but do contain some differences. This lab step will walk you through creating an Alarm from the EC2 console. Once created, you will also add another layer to the alarm: the ability to perform EC2 actions when the alarm is triggered. For example, automatically reboot an instance that is struggling with high CPU utilization.

You may find some AWS or custom metrics contain interesting enough information that you would like to share it with others. For example, an instance that occasionally runs with high CPU usage, or appears to have a memory leak, or a disk volume that fills up too quickly, etc. There is an easy to use but often overlooked feature in the CloudWatch console that provides an easy way to share this information.


## Manage Message Queues Using Amazon SQS
Link: https://cloudacademy.com/lab/manage-message-queue-amazon-sqs/

Amazon Simple Queue Service (Amazon SQS) offers a fast, reliable and scalable queues for storing messages. By using Amazon SQS, you can move data between distributed components of your applications that perform different tasks without losing messages or requiring each component to be always available.

A queue is a temporary repository for messages that are awaiting processing. The queue acts as a buffer between the component producing and saving data, and the component receiving the data for processing. This means the queue resolves issues that arise if the producer is producing work faster than the consumer can process it, or if the producer or consumer are only intermittently connected to the network. SQS ensures delivery of each message at least once, and supports multiple readers and writers interacting with the same queue. A single queue can be used simultaneously by many distributed application components, with no need for those components to coordinate with each other to share the queue.

You can create an SQS queue using the AWS Management Console.

In this Lab Step you will use the AWS Management Console to poll for SQS messages. Polling is how Amazon SQS tracks messages sent to a particular queue.


5. Review the available properties in the Message Details modal:

    Message ID
    Size in bytes
    MD5 checksum of the body
    Sender Account ID (you can allow other AWS accounts to push messages on your queue)
    Sent Date and Time
    First Received
    Receive Count
    Message Attribute Count


Introduction

Part of this Lab requires a terminal with the AWS CLI installed. In this Lab, you will use AWS Cloud9 as a terminal and CLI. AWS Cloud9 is an Integrated Development Environment (IDE) that runs in a browser. You can write, run, and debug your code in Cloud9. The Cloud9 environment can be shared with multiple collaborators to work on the same project at the same time. You can view the changes collaborators are making in real-time, making it a useful tool for pair programming.

Cloud9 environments are backed by either an EC2 instance or by any ssh-connected machine with Python installed, including machines outside of AWS. The IDE includes an integrated terminal connected to the machine that backs the Cloud9 environment. For EC2 Cloud9 environments, Cloud9 will automatically put the EC2 instance to sleep after a period of inactivity to save you the cost of running the instance when you are not using Cloud9. There is no additional cost to using Cloud9 beyond the cost of running the instance.


 Take a minute to explore the Cloud9 interface. The default view shows three main panels:

    Environment: Left panel that shows the file tree for your project
    Editor: Upper panel where you can open tabs for editing files. It displays a Welcome page by default.
    Terminal: Lower panel with a terminal connected to the EC2 instance backing the Cloud9 environment.

 The default layout is only a suggestion. You can fully configure the layout. For example, you can add, remove, resize, and split panels; open files in the lower pane; and open terminals in the upper panel.

There is also a hidden pane on the right that can be opened by clicking on any of the Collaborate, Outline, AWS Resources, or Debugger tabs. The Collaborate tab shows the members of the Cloud9 environment, whether they are online or not, and their permissions, either read (R) or read/write (RW). You can enter messages in the chat log, and also navigate to the active cursor of other members by right-clicking their name and choosing show location. The Outline tab shows locations you can jump to in the current file, for example, the lines where functions are defined in code. AWS Resources is useful for managing Lambda functions. The Debugger is exactly what you would expect in an IDE - a tool for stepping through the code and inspecting values at runtime.

```bash
aws sqs list-queues
aws sqs list-queues --output text --query "QueueUrls"
# Store the Queue URL in a shell variable named queue_url 
# The command is surrounded with $(...) to store the result of the command
queue_url=$(aws sqs list-queues --output text --query "QueueUrls")
# Send a message to the queue (use $ to substitute the value stored in the variable)
aws sqs send-message --queue-url $queue_url --message-body "I'm an SQS message from the CLI"
aws sqs receive-message --queue-url "$queue_url"
aws sqs delete-message --queue-url $queue_url --receipt-handle YOUR_RECEIPT_HANDLE

```

You should notice that even though you've requested and successfully received your message using the CLI, the message continues to display in the SQS dashboard. This is because Amazon SQS doesn't automatically delete a message after it has been received, in case you don't successfully receive or process the message (for example, if your application fails or you lose connectivity). To delete a message you would send a separate delete request which acknowledges that you no longer need the message because you've successfully received and processed it.



## Serve your files using the CloudFront CDN
Link: https://cloudacademy.com/lab/serve-your-files-using-cloudfront-cdn/

Lab description

Amazon CloudFront is a content delivery network (CDN) service. You can speed up the delivery of static files using HTTP or HTTPS protocols. Each CloudFront distribution has a unique cloudfront.net domain name that can be used to reference objects through the global network of edge locations.

AWS CloudFront uses a global network of edge locations for content delivery. There are 20 locations in the USA, 16 locations in Europe, 13 in Asia, 2 in Australia, and 2 in South America. You can also monitor and receive notifications on the operational performance of CloudFront distributions using CloudWatch, and track trends in data transfer and requests checking the usage charts.

CloudFront is a powerful service and, during this lab, you will learn to create a fully functional CloudFront distribution using an S3 bucket as the origin. 
Learning Objectives

Upon completion of this beginner level lab, you will be able to:

    Create an Amazon S3 bucket
    Create an Amazon CloudFront distribution
    Upload a demo website to an S3 bucket
    Serve S3 content through a CloudFront distribution
    Disable and delete a CloudFront distribution

Note: Cloudfront may take up to 25 minutes to deploy your distribution. You will need to wait until it is deployed to complete the laboratory.


 Introduction

Amazon CloudFront is a content delivery service (CDN). In Amazon CloudFront, the content is organized into distributions.

Each distribution has a unique cloudfront.net domain name (e.g. cdn123.cloudfront.net) that can be used to reference objects through the global network of edge locations. 

To use Amazon CloudFront:

    Store the original versions of your files on one or more origin servers. An origin server is the location of the definitive version of an object. Origin servers could be an Amazon S3 bucket, an Amazon EC2 instance, an Elastic Load Balancer, or another remote server.
    Create a distribution to register the origin servers with Amazon CloudFront.
    Use your distributionâ€™s domain name in your web pages, media player, or application. When end users request an object using this domain name, they are automatically routed to the nearest edge location for high-performance delivery of your content.

 Amazon CloudFront minimizes end-user latency by delivering content from its entire global network of edge locations. Price Classes let you reduce your delivery prices by excluding Amazon CloudFrontâ€™s more expensive edge locations from your Amazon CloudFront distribution. In these cases, Amazon CloudFront will deliver your content from edge locations within the locations in the price class you selected and charge you the data transfer and request pricing from the actual location where the content was delivered.

The time required for deploying a new CloudFront distribution also depends on the number of selected Edge Locations. Selecting all edge locations will take longer to deploy.

 
 Look at the URL in the address bar, it will be similar to:

    https://calabs-abc123.s3-us-west-2.amazonaws.com/gallery/index.html

The URL of any S3 object follows this template:

    https://<bucket-name>.s3-us-west-2.amazonaws.com/<object-prefix>/<object-name>

Note: You may encounter older S3 buckets using the following URL format:

    https://s3-<region>.amazonaws.com/<bucket-name>/<object-prefix>/<object-name> 

In 2020 AWS changed new buckets to use the current format (bucket name as a part of the hostname).


## Create Your First Amazon EC2 Instance (Windows)
Link: https://cloudacademy.com/lab/create-your-first-amazon-ec2-instance-windows/

Lab description

Amazon EC2 allows you to launch different types of cloud instances and pay for them with a pay-per-use approach. With Amazon EC2 you can create new servers in a few minutes and use different images (AMI) to personalize them. AWS provides several Microsoft Windows Server AMIs that enable you to run almost any compatible Windows-based solution. You can use Windows-based applications, websites, and web-services written in .NET, for data processing, media transcoding, and any other task requiring Windows software.

By completing this Hands-on Lab, you will be able to launch and access your first Amazon EC2 instance running Microsoft Windows Server.
Lab Objectives

Upon completion of this Lab you will be able to:

    Use the AWS Management Console to create an EC2 instance using the Windows operating system
    Use the AWS Management Console to work with EC2 Key Pairs
    Access the EC2 instance using Remote Desktop

Lab Prerequisites

You should be familiar with:

    Some familiarity with Windows and Remote Desktop Connection is helpful but not required
    Some familiarity with the AWS Management Console is helpful but not required


Amazon EC2 uses publicâ€“key cryptography to encrypt and decrypt login information. Publicâ€“key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to your instance, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. 

By default, most Linux instances have no password and you use a keypair to log in using SSH. Windows instances have an auto-generated Administrator password that you can retrieve using the keypair file.


## Monitoring AWS CloudTrail management events with Amazon CloudWatch Logs
Link: https://cloudacademy.com/lab/monitoring-aws-cloudtrail-events-amazon-cloudwatch/

Lab description

AWS CloudTrail is a service that enables you to log and monitor activity in an AWS account. CloudTrail events are delivered to an S3 bucket and are also available for viewing from the CloudTrail console. AWS CloudTrail can be configured to send events to CloudWatch where log events can be further monitored and audited for compliance.

In a use case that involves tracking and auditing account activity, AWS CloudTrail and Amazon CloudWatch have the following responsibilities:

    AWS CloudTrail tracks API activity within an AWS account
    Amazon CloudWatch Logs stores event logs and is capable of triggering downstream notifications and processes using alarms

In this lab, you will create a CloudTrail trail integrated with CloudWatch Logs. You will configure the CloudWatch metric filters and alarms to send notifications based on certain management events.
Learning objectives

Upon completion of this beginner-level lab, you will be able to:

    Configure a trail to capture management events and deliver log files to an S3 bucket
    Integrate the trail with CloudWatch Logs
    Generate management events by launching an Amazon EC2 instance
    Configure a CloudWatch metric filter and alarm

Intended audiences

    Candidates for the AWS Certified Security - Specialty Certification
    Cloud Architects
    Software Engineers


By default, AWS records the last 90 days of events within an account in the CloudTrail event history. However, the default events do not support triggering alerts, event metrics, and long-term storage. 

In this lab step, you will configure a CloudTrail trail by creating an Amazon S3 bucket for log storage, and utilizing an existing IAM role to grant CloudTrail the necessary permissions. You will also configure CloudWatch logs on your trail to be able to monitor your trail logs. 


Notice the following details:

    Trail logging: Logging will be displayed in green, signifying that logging is currently enabled
    Trail log location: This link will redirect you to the S3 bucket location of the event logs for this account
    Multi-region trail: A multi-region trail logs events across all regions and stores them in folders that match the region name
    Last log file delivered: A timestamp will appear after CloudTrail has recorded its first trail log event

Note: CloudTrail can take up to 5 minutes to deliver the log files to the S3 bucket. Continue to refresh this page until a timestamp is present below Last log file delivered.


AWS CloudTrail is not a real-time service. Account event logs are delivered every 5 minutes when the trail is active, with up a potential delay of up to 15 minutes.

If the CloudTrail/ directory does not contain any objects, you will need to wait a few more minutes for the logs to appear. 

Refresh the S3 CloudTrail directory list until a directory for us-west-2 appears:


Once CloudTrail event logs are available in CloudWatch Logs, developers have the ability to trigger CloudWatch alarms and downstream notifications using trail events.

In this lab step, you will configure a CloudWatch metric filter and alarm to respond to EC2 events and send a notification once a threshold has been reached.

 ```bash
 {($.eventName = "StopInstances" ) || ( $.eventName = "TerminateInstances")}
 ```

 This filter will track the StopInstances and TerminateInstances management events that occur in this account. The metric value for each event is set to 1, which means either a stopped or terminated instance event will increment the metric value equally. 