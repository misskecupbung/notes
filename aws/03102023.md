### data store, networking, security migration, architect to scale, business continuity, deployment and operation management, and cost management

4 domains of SAP: 
1. design solution for orhs cpmplexity
2. design for new solution
3. continuous improvement for existing solutions
4. accelerate workload migration and modernation

Design solution for orgs complexity
1. connecting on-prem to aws, multiple vpc/multiple data center, for hybrid
2. multi account orgs
3. cost&usage, iam, security, backup

design for new solution
1. operational excellence (iac, config management)
2. security
3. reliability (failover, elasticity)
4. efficiency (serverless, event-driven arch)
5. cost optimization (same 4)
6. sustainability (Same 4)

continuous improvement for existing solutions 
1. measure (cloud watch)
2. react (event bridge, lambda)
3. improve 
4. security

accelerate workload migration and modernization
1. on-repmise data
2. migration serivces
3. best-fit cloud solution

bloom's taxonomy
1. understand: recalling and explaining facts and concepts
2. apply: using the information you've learned in new situations
3. evaluate: drawing connection between ideas and justifying strategic decisions
4. create: producing new or original work

preparing for every facet:
1. aws knowledge
2. contextual reasoning
3. time management
4. endurance
5. acting under pressure
6. comprehension

section breakdown
1. section concepts
2. aws services
3. hands-on lab
4. exam and pro tips
5. domain challenges
6. section quiz

key to success
1. course completion
2. supplemental materials
3. seek out experience
4. get involved
5. plan and prepare

data store: section introduction
A. Design solution for organizational complexity
1. deploying encryption strategies for data at rest and in transit
2. data backup and restoration

B. Design for new solutions
1. storage options on aws
2. storage tiering and data transfer costs
3. configure data and database replication
4. operating and maintaining high-availability architectures (e.g: application failover, database failover)
4. aws storage services and replication strategies
5. purpose-built databases

C. Continuos Improvement for existing solution
1. data retention, data sensitivity, and data regulatory requirements
2. data replication methods

D. accelerate workload migration and modernization
1. databases
2. select the appropriate database platform
3, selecting the appropriate storage services

data store
1. concept
2. data services
3. hands-on lab
4. tips
5. challenge
6. quiz

Data storage concept
1. data persistence
data is durable and sticks around after reboot, restarts, or power cycle
e.g: glacier, rds

2. transient data store
data is just temporarily stored and passed along to another process or persistent store
e,g: sqs, sns

3. ephemeral data store
data is lost when stopped
e.g: ec2 instance store, memchaced

iops (input/output operation per second)
- mesaure of how fast we can read and write to a device

throughput
- measure of how much data dan be moved at a time

consistency is far better than rare moments of greatness - scott ginsberg

consistency models
A. acid 
- atomic: transactions re all or nothing
- consistent: transaction must be valid
- isolated: transactions can't mess with one another
- durable: competed transaction must stick around

B. base
- basic availability: values availability even if stale
- soft-state: might not be instantly consistent across stores
- eventual consistenty: will achieve consistency at some point

Amazon s3
1. one of the first aws service introduced back in 2006
2. s3 is an object storage
3. used in other aws services - directly and behind-the-scenes
4. maximum object size is 5tb; largest object in a single put is 5gb
5. recommended to use multi-part uoloads if larger than 100mb

s3 has different facets of security:
A. resource-based (object acl, bucket policy)
b. user-based (iam policies)
c. optional mfa before delete

s3://bucket/folder/a.pdf : a key not a file path

it uniquely identifies that record in that document file store

s3 data protection
versioning:
1. new version with each write
2. enables "roll-back" and "un-delete" capabilities
3. old versions count as billable size until they are permanently deleted
4. integrated with lifecycle management

s3 data protection
optionally require mfa:
1. safeguard against accidental deletion of an object
2. change the versioning state of your bucket

s3 cross-region replication
1. security
2. compliance
3. latency

s3 storage class

s3 intelligent tiering archive:
1. automatically moves data to glacier or deep glacier

s3 lifecycle management
1. optimize storage cost
2. adhere to data retention policies
3. keep s3 volumes well-maintained

s3 analytics
1. data lake concept: athena, redshift spectrum, quicksight
2. iot streaming data repository: kinesis firehose
3. machine learning and ai storage: rekognition. lex mxnet
4. storage class analysis: s3 management analytics

s3 encryption at rest
1. SSE-S3: Use s3's existing encryption key for AES-256
2. SSE-C: Upload your own AES-256 encryption key which s3 will use when it writes the objects
3. sse-kms: use a key generated and managed by aws key management service
4. client-side: encrypt object using your own local encyprtion process before uploading to s3 (i.e: pgp, gpg)

more nifty s3 tricks:
1. transfer acceleration: speed up data uploads using cloudfront in reverse
2. requester pays: the requester rather than the bucket owner pays for requests and data transfer
4. tags: assign tags to objects for use in costing, billing, security, etc
5. events: trigger notification to sns, sqs or lambda when certain events happen in your bucket
6. static web hosting: simple and massively scalable static web hosting

managing permissions in s3:
1. bucket policies vs iam
2. sharing across accounts
3. connecting to resources
4. things to take away

bucket policies vs iam

bucket policies
1. resource-based policies: defined at the resource itself
2. iam: identity-based policy

iam user will access based resource-based first, then identity-based

connecting to resouece:
1. igw (through internet)
2. eni (s3 gateway endpoint)

managing permissions in s3. Three things to take away
1. give access via bucket policy or iam. Explicitly allow or deny access to principles with bucket policies. IAM roles can grant access if it's not denied by bucket policies
2. give secure access accross account. Using trust policies and iam roles, grant principles access to your s3 objects whether or not they are in the same aws account
3. improve efficiency with s3 endpoint. Instead of sending data through the public internet, you may find cost and performance savings by using an s3 endpoint

amazon glacier
1. cheap, slow to respond, seldom accessed
2. cold storage
3. used by aws storage gateway virtual tape library
4. integrated with aws s3 via lifecycle management
5. faster retrieval speed options if you pay more
6. for archive solution

amazon glacier use different api than s3
iam > glacier vault
1. archive, file zip, tar. Max size 40tb. Immutable
2. Glacier vault lock. Different than vault access policy. Immutable

glacier vault < (initiate vault lock) < glacier vault lock 
24 hour timeout

Amazon EBS
1. Think virtual hard drives
2. Can only be used with ec2
3. Tied to a single az
4. Variety of optimized choices for iops, throughput and cost
5. snapshot are great

Instance store:
1. temporary
2. ideal for caches, buffers, work areas
3. data goes away when ec2 is stopped or terminate

amazon ebs snapshots
1. cost-effective and easy backup strategy
2. share data sets with other users or accounts (using snapshot)
3. migrate a system to a new az or region
4. convert unencrypted volume to an encrypted volume

Amazon Data Lifecycle Manager
1. Schedule snapshots for volumes or instances every X hours
2. Retention rule to remove stale snapshots

EFS
1. Implementation of NFS file share
2. Elastic storage capacity, and pay for only what you use (in contrast to ebs)
3. Multi-az metadata and data storage
4. configure mount-points in one, or many azs
5. Can be mounted from on-premises system
6. Alternatively use Amazon DataSync to sync between efs and on-premise

FSx: AWS file sharing service which compatible with windows file share

FSx
1. distributed file system
2. provides non-NFS options for file sharing
3. Most commonly used for windows file services integration with AWS
4. Takes advantage of the scalability and availability enabled by aws

The four flavors of FSx:
1. FSx for NetApp ONTAP
2. FSx for OpenZFS
3. FSx for Windows File Server
4. FSx for Lustre

How does fsx work?
1. Managed Microsoft AD
2. ENI

To mount the file system

Use Case - FSx for Windows File Server
1. Windows line-of-business applications
2. Windows content management
3. Media processing workflows
4. Windows data analytics

Use Case - FSx for Lustre
1. high-performance distributed applications
2. High-performance compute clusters
3. Can reliably interact with thousands of ec2 instances
4. Big data and machine learning apps

3 things to take away:
1. fsx covers several file share use cases
- most commonly used for windows file server file share, fsx can also be used for ONTAP, OpenZFS, and Lustre with AWS integration

2. Use FSx for Windows File Server
- For SMB-based file shares that EFS is not compatible with, FSx is a great alternative. AWS handles management of the servers, so you only have to worry about access.

3. FSx for Lustre Uses Modern HPC Storage
- FSx for Lustre strategically organizes data on SSDs to optimize the most used data for fast access, designed for high performance

Amazon Storage Gateway
1. Virtual machine that you run on-premises with VMWare or HyperV OR via a specially configured Dell hardware appliance
2. Provides local storage resources backed by AWS S3 and Glacier
3. Often used in disaster recovery preparedness to sync to AWS
4. Useful in cloud migrations

Amazon Storage Gateway Modes:
1. New Name: File Gateway, Old: None. Iface: NFS, SMB. Function: Allow on-prem or ec2 instances to store objects in s3 via nfs or smb mount point
2. New Name: volume gateway stored mode, Old name: gateway-stored volume, Iface: iscsi, Function: async replication of on-premise data to s3,
3. New Name: volume gateway cached mode, Old name: gateway-cached volume, Iface: iscsi, Function: primary data stored in s3 with frequently access data cached locally on-prem,
4. New Name: tape gateway, Old name: gateway-virtual tape library, Iface: iscsi, Function: virtual media changer and tape library for use with existing backup software,

database on ec2:
1. run any database with full control and ultimate flexibility
2. must manage everything like backups, redudancy, patching, scale
3. good option if you require a database not yet supported by rds, such as ibm db2 or sap hana
4. good option if it is not feasible to migrate to aws-managed database

RDS
1. Managed database option for mysql, maria, postgresql, microsoft sql server, oracle, and mysql-compatible aurora
2. best for structured, relational data store needs
3. aims to be drop-in replaceemnt for existing on-prem instances of same databases
4. automated backup and patching in customer-defined maintenance windows
5. push-button scaling, replication, and redudancy

Amazon RDS Anti-pattern
If you need, dont use rds, instead use
1.  lots of large binary objects (blobs), s3
2. automated scalability, dynamodb
3. name/value data structure, dynamodb
4. data is not well structured or unpredictable, dynamodb
5. other database platforms like ibm, db2 or sap hana, ec2
6. complete control over the database, ec2

amazon rds
1. multi-az rds
2. read-replicas service regional users
.

note for mysql: non-transactional storage engines like myISAM don't support replication; you must use innodb (or xtradb on maria)

mariadb is an open-source fork of mysql. mysql behaves similar

amazon rds replica type:
1. sync
2. async

1. one az failes
2. stand-by in another az assumes role of master
3. read replicas keep on keeping on
.
note: mariadb rds db can have one stand-by instance and multiple read replicas

1. whole region failed
2. read replica promoted to stand-alone (single az)
3. single az reconfigred to multi-az

introducing aurora
1. the most advanced rds solution on aws
2. fully-managed rds service for mysql and postgresql databases
3. multi-az availability by design
4. aws-managed autoscaling
5. easier multi-region replication

amazon auorora - read replicas
1. one main instance and upto 15 read instances
2. data is held on data stores distributed accross the azs and its independent from the instances that you provision
3. so it can scale in and out independently based on the amount of data that you're storing
4. main instance handles all right traffic writing to all data copies
5. one data is written, it usually available to read within 100 ms
6. when the main instance is fails, one of your read instance will automatically be promoted
7. we don't have to worry about switching endpoint over to target a different instance 

amazon aurora - global databases
1. multiple-region replication
2. high availability and disaster recovery
3. one primary region and up to five regions
4. data replicates from primary to secondary regions with low latency
5. leverages storage-level replication for transferring data
6. secondary region can be promoted in aase of outage

auorora serverless

1. aurora standard. Predictable, consistent workload
2. aurora serverless. highly variable workload

amazon aurora serverless
1. ACU stands for Aurora Capacity Units, which equates to about 2 GiB of memory, cpu, and networking. You can set minimum or maximum ACU for a cluster from 0.5 to 128.

Amazon Aurora Use Cases
1. MySQL and PostgreSQL databases
2. inherently highly available and resilient to failure
3. aurora serverless is available for highly variable workloads
4. low operational overhead

Amazon Aurora - 3 things to take away
1. designed for high availability
- aurora stores data redundantly within and across availability zones. failover and data storage scaling is handled automatically

2. read replicas allow for low latency
- read replicas can be provisioned across availability zones to distribute read traffic. Global databases can be used to span across regions.

3. use serverless for variable workloads
- amazon aurora serverless v2 allow you to provision instances that automatically scale their memory, cpu, and network capacity

Amazon DynamoDB
1.  managed, multi-az nosql data store with cross-region replication option
2. defaults to eventual consistency reads but can request strongly consistent read via SDK parameter
3. priced on throughput, rather than compute
4. provision read and write capacity in anticipation of need
5. autoscale capacity adjusts per-configured min/max levels
6. on-demand capacity for flexible capacity at a small premium cost
7. achieve acid compliance with dynamodb transactions

Relational vs NoSQL
1. relational db is best when the data is structured very well relational
2. nosql databases are designed to excel at managing name value-pairs. 
3. nosql record is usually self-contained and doesn't have to relate to any other table to make sense.

amazon dynamodb
1. name
2. value
3. atrribute (combined by name and value)
4. item: the whole collection of name and value
5. table: collection of items

amazon dynamodb
1. primary key: unique, partition key
2. for access the table, we only need to know the primary key
3. dynamo db is going to use the primary key value to create an internal hash

amazon dynamodb
1. when we are using two primary keys, first known as a partition key, the second one is sort key

amazon dynamodb - secondary indexes
1. global secondary index: partition key and sort key can be different from those on the table. 
2. local secondary index: same partition key as the table but different sort key

1. there is a limit to the number of indexes and attributes per inde
2. indexes take up storage space

1. global secondary index: when you want a fast query of attributes outside the primary key: without having to do a table scan (read everything sequentially)
2. local secondary index: when you already know the partition key and want to quickly query on some other attribute

global secondary index - example
1. suppose we created a global secondary index using customerNum
2. we cloud query by customer number at light-speed

local secondary index - example
1. suppose we created a local secondary index using materialNumber
2. we could query by sales order number and material number at light-speed

DocumentDB is an aws-native document data storage service.
1. NoSQL JSON document database service
2. compatibility with mongodb api
3. aws-managed autoscaling
4. supports write-then-read consistency

what is a document
1. a document is a json blob
2. its designed to be retrieved all at once and contain data that's intuitively structured for developers to use in their application

JSON format
1. document databases store data in json. this allows a natural schema which can make development more intuitive

flexible schema
1. document data is nosql data which has a flexible schema. this allows developers to iterate quickly
2. allows developers to add fields or remove them without worrying about breaking some rigid schema like in an nosql database

amazon documentdb cluster is similar with amazon aurora

amazon documentdb use cases
1. nosql document database
2. user profiles
3. real-time big data
4. content management

amazon documentdb - 3 things to take away
1. built to scale. like amazon aurora, documentdb is built from the ground up to scale data storage and handle multi-az and multi-region read replicas
2. store jsob document data. JSON Document databases allow for flexible and intuitive schemas, which allows for iterative and natural development
3. mongodb compatibility. watch for schenarios asking you to emulate mongodb or migrate document databases. documentdb may be a great choice

amazon redshift:
1. fully-managed, clustered peta-byte scale data warehouse
2. extremely cost-effective as compared to some other on-premise data warehouse platforms
3. postgresql compatible with jdbc and odbc drivers available; compatible with most BI tools out of the box
4. Features parallel processing and columnar data stores which are optimized for complex queries
5. Option to query directly from data files on s3 via redshift spectrum

Data lake
1. query raw data without extensive pre-processing
2. lessen time from data collection to data value
3. identify correlations between disparate data sets

data lake:
transactions logs, sensor readings, social media stream, weather data < amazon redshift spectrum < analytics tool

Data ecosystems
1. source: s3, data lake, redshift, operational databases
2. amazon glue, amazon athena,
3. quicksight, sagemaker

ETL:
1. Extract: By extracting only the data relevant to your insights, we can get more focused and more accurate insight
2. Transform: clean, standardize, deduplicate, transform, combine, sort, etc
3. Load: load to app/data source

AWS Glue
1. discover, prepare, and integrate data
2. discover data with Glue crawlers
3. prepare data with glue data catalogs and glue etl jobs
4. integrate data from disparate sources
5. serverless, aws handles scaling for you

AWS Glue
1. glue crawler, collect data from data source (s3, redshift, rds, db in ec2 instance)
2. store the metadata / relevant data in glue data catalog. can be leverage directly by services like emr, redshift, or athena
3. glue etl jobs: transform data defined in catalog and load it into lake formation, redshift, s3, cloudwatch (maybe the last dest, or can be deliver to data consumer)

AWS Glue Data Quality
1. Predefined rules. Glue data quality provides data quality recommendations and predefined rules to help you ensure the quality of your glue data.
2. metrics and alerts. using glue data quality, you can set up alerts and data quality metrics to provide confidence in the data you are leveraging.

Amazon Athena
1. Analyzing petabyte-scale data where it lives. 
2. serverless big data querying service
3. designed to create databases from s3 objects
4. uses standard sql to create queries
5. can run spark code with apache spark on amazon athena

Amazon Athena
1. data source: s3, lake formation, operational database
2. athena: sql query to gain insights . define federated queries to analyze data from over 25 different data sources
3. data consumer: bi&analytics, ml solutions,

3 things to take away - preparing data for analysis
1. most data ecosystems require some etl. Collecting data is only valuable so long as you can draw insights from 